backend {
  default = slurm

  providers {
    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        runtime-attributes = """
        Int cpus = 8
        Int runtime_minutes = 1380
        Int memory_gb = 60
        String? docker
        String queue = "work"
        """

        submit = """
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              -t ${runtime_minutes} \
              -p ${queue} \
              ${"-c " + cpus} \
              --mem=${memory_gb}G \
              --wrap "/bin/bash ${script}"
        """

        submit-docker = """
            # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
            # based on the users home.
            export SINGULARITY_CACHEDIR=/scratch/pawsey0933/cfolland/giraffe/singularity_cache
            CACHE_DIR=$SINGULARITY_CACHEDIR
            export SINGULARITY_TMPDIR=/scratch/pawsey0933/cfolland/giraffe/singularity_tmp
            TMP_DIR=$SINGULARITY_TMPDIR
            # Make sure cache dir exists so lock file can be created by flock
            mkdir -p $CACHE_DIR  
            LOCK_FILE=$CACHE_DIR/singularity_pull_flock
            mkdir -p $TMP_DIR
            # Create an exclusive filelock with flock. --verbose is useful for 
            # for debugging, as is the echo command. These show up in `stdout.submit`.
            flock --verbose --exclusive --timeout 900 $LOCK_FILE \
            singularity exec --containall docker://${docker} \
            echo "successfully pulled ${docker}!"

            # Build the Docker image into a singularity image
            # We don't add the .sif file extension because sandbox images are directories, not files
            DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
            IMAGE=${cwd}/$DOCKER_NAME
            singularity build --sandbox $IMAGE docker://${docker}

            # Submit the script to SLURM
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              -t ${runtime_minutes} \
              ${"-c " + cpus} \
              --mem=${memory_gb}G \
              --wrap "module load singularity/4.1.0-slurm; singularity exec --cleanenv --no-mount /etc/slurm --no-mount /usr/share/zoneinfo --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}"
        """

        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"

        # Add this section to specify symlinks instead of hard links
        filesystems {
          local {
            localization: [
              "cached-copy", "copy"
            ]
          }
        }
      }
    }
  }
}

system {
  # Actor timeout configuration
  workflow-store {
    coordinator-timeout = 60 minutes
  }

  job-execution-token-dispenser {
    coordinator-timeout = 60 minutes
  }

  workflow-log-copy-router {
    coordinator-timeout = 60 minutes
  }

  service-registry {
    coordinator-timeout = 60 minutes
  }

  io-proxy {
    coordinator-timeout = 60 minutes
  }

  call-cache-write-actor {
    coordinator-timeout = 60 minutes
  }
}

call-caching {
  enabled = true
  invalidate-bad-cache-results = true
}